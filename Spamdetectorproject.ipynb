{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNL+kREHYM5AgJnyHVRADA0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManasiA/MLproject/blob/main/Spamdetectorproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas scikit-learn nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7msWC3W8B7ac",
        "outputId": "5baa3485-94ea-4fe7-f6ee-1b48e7c2b5a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3kw9RAcCHQI",
        "outputId": "3deeae95-cb7d-4394-bb45-9839a132e755"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-31 05:50:51--  https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 503663 (492K) [application/octet-stream]\n",
            "Saving to: ‘spam.csv’\n",
            "\n",
            "spam.csv            100%[===================>] 491.86K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-07-31 05:50:51 (10.0 MB/s) - ‘spam.csv’ saved [503663/503663]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "data = data[['v1', 'v2']]  # Keep only label and text columns\n",
        "data.columns = ['label', 'text']  # Rename columns"
      ],
      "metadata": {
        "id": "CU9eWJrsCKR-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head())#for display 1st 5 rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzW7MN-cCpjh",
        "outputId": "b33bd539-70cf-4e90-adb7-6861c03ffe22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  label                                               text\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check counts of spam vs ham\n",
        "print(\"\\nSpam vs Ham Counts:\")\n",
        "print(data['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uQSP124C0zp",
        "outputId": "f10de57c-f230-45ed-c94f-6c375c628fb4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Spam vs Ham Counts:\n",
            "label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CrT7dSGC7sv",
        "outputId": "cc054e2f-e0ad-4c89-9a30-157a4548dda3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing\n",
        "data['processed_text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# Display cleaned text\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAgZoGV4DFlg",
        "outputId": "3dd6e109-87bc-4d50-ed95-4f0c1c88bf1c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  label                                               text  \\\n",
            "0   ham  Go until jurong point, crazy.. Available only ...   \n",
            "1   ham                      Ok lar... Joking wif u oni...   \n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
            "3   ham  U dun say so early hor... U c already then say...   \n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
            "\n",
            "                                      processed_text  \n",
            "0  go jurong point crazi avail bugi n great world...  \n",
            "1                              ok lar joke wif u oni  \n",
            "2  free entri 2 wkli comp win fa cup final tkt 21...  \n",
            "3                u dun say earli hor u c alreadi say  \n",
            "4          nah dont think goe usf live around though  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data['processed_text']  # Features (processed text)\n",
        "y = data['label']           # Labels (spam/ham)\n",
        "\n",
        "# Split into 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "a-UYYvdUDvUH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Fit on training data, transform both train & test\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "QZ3-kcx_Ee3M"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Initialize and train the model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Confusion matrix (shows true vs predicted labels)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDjctkkiFJNp",
        "outputId": "802e26fd-6cfe-43dc-ea26-b5cd62ad2e3d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.97\n",
            "\n",
            "Confusion Matrix:\n",
            "[[965   0]\n",
            " [ 33 117]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_spam(email_text):\n",
        "    processed = preprocess_text(email_text)\n",
        "    vector = vectorizer.transform([processed])\n",
        "    prediction = model.predict(vector)[0]\n",
        "    probability = model.predict_proba(vector)[0]\n",
        "    return prediction, probability\n",
        "\n",
        "# Test with a sample email\n",
        "sample_email = \"WIN A FREE IPHONE! CLICK NOW!\"\n",
        "prediction, prob = predict_spam(sample_email)\n",
        "print(f\"Prediction: {prediction}\")\n",
        "print(f\"Probabilities (ham, spam): {prob}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97SCDPdPFaly",
        "outputId": "3a901b67-e196-4035-c233-dfc3fccfd356"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: spam\n",
            "Probabilities (ham, spam): [0.42332138 0.57667862]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save model and vectorizer\n",
        "joblib.dump(model, 'spam_model.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
        "\n",
        "# Later, you can load them with:\n",
        "# model = joblib.load('spam_model.pkl')\n",
        "# vectorizer = joblib.load('tfidf_vectorizer.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcoWBwGPFncB",
        "outputId": "0b34a745-4321-4626-94bf-a52b2443de79"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}